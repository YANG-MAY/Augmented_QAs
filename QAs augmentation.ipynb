{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synonym Replacement\n",
    "\n",
    "import json\n",
    "import nlpaug.augmenter.word as naw\n",
    "\n",
    "# Read the input data from a JSON file\n",
    "with open('/Users/yangl/Downloads/answers.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Perform data augmentation for each entry\n",
    "augmented_data = []\n",
    "for entry in data['data']:\n",
    "    paragraphs = entry['paragraphs']\n",
    "    augmented_paragraphs = []\n",
    "    for paragraph in paragraphs:\n",
    "        qas = paragraph['qas']\n",
    "        augmented_qas = []\n",
    "        for qa in qas:\n",
    "            original_question = qa['question']\n",
    "            answer = qa['answers'][0]['text']\n",
    "\n",
    "            # Apply Synonym Replacement using nlpaug\n",
    "            augmenter = naw.SynonymAug(aug_src='wordnet')\n",
    "            augmented_questions = augmenter.augment(original_question, n=5)\n",
    "\n",
    "            augmented_qa = {\n",
    "                'question': original_question,\n",
    "                'augmented_questions': augmented_questions,\n",
    "                'answers': [{'text': answer}]\n",
    "            }\n",
    "            augmented_qas.append(augmented_qa)\n",
    "\n",
    "        augmented_paragraph = {\n",
    "            'qas': augmented_qas,\n",
    "            'context': paragraph['context'],\n",
    "            'document_id': paragraph['document_id']\n",
    "        }\n",
    "        augmented_paragraphs.append(augmented_paragraph)\n",
    "\n",
    "    augmented_entry = {'paragraphs': augmented_paragraphs}\n",
    "    augmented_data.append(augmented_entry)\n",
    "    \n",
    "    with open('/Users/yangl/Downloads/augmented_data_3.json', 'w') as file:\n",
    "        json.dump(augmented_data, file, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# synonym replcement with sentense shuffling \n",
    "import json\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import nlpaug.augmenter.word as naw\n",
    "\n",
    "# Load pre-trained BERT model and tokenizer\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name)\n",
    "\n",
    "# Read the input data from a JSON file\n",
    "with open('/Users/yangl/Downloads/answers.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Perform data augmentation for each entry\n",
    "augmented_data = []\n",
    "for entry in data['data']:\n",
    "    paragraphs = entry['paragraphs']\n",
    "    augmented_paragraphs = []\n",
    "    for paragraph in paragraphs:\n",
    "        qas = paragraph['qas']\n",
    "        augmented_qas = []\n",
    "        for qa in qas:\n",
    "            original_question = qa['question']\n",
    "            answer = qa['answers'][0]['text']\n",
    "\n",
    "            # Apply Synonym Replacement using nlpaug\n",
    "            augmenter_question = naw.SynonymAug(aug_src='wordnet')\n",
    "            augmented_questions = augmenter_question.augment(original_question, n=5)\n",
    "\n",
    "            # Augment the answer using Sentence Shuffling\n",
    "            augmenter_answer = naw.ContextualWordEmbsAug(model_path='bert-base-uncased', action='substitute')\n",
    "            augmented_answer = augmenter_answer.augment(answer)\n",
    "\n",
    "            # Check answer coherence\n",
    "            def check_answer_coherence(answer, context):\n",
    "                text = f\"{answer} {context}\"\n",
    "                inputs = tokenizer(text, truncation=True, padding=True, return_tensors='pt')\n",
    "                outputs = model(**inputs)\n",
    "                predicted_label = outputs.logits.argmax().item()\n",
    "                return predicted_label\n",
    "\n",
    "            coherence_label = check_answer_coherence(augmented_answer, paragraph['context'])\n",
    "            if coherence_label == 1:\n",
    "                augmented_qa = {\n",
    "                    'question': original_question,\n",
    "                    'augmented_questions': augmented_questions,\n",
    "                    'answers': [{'text': augmented_answer}]\n",
    "                }\n",
    "                augmented_qas.append(augmented_qa)\n",
    "\n",
    "        augmented_paragraph = {\n",
    "            'qas': augmented_qas,\n",
    "            'context': paragraph['context'],\n",
    "            'document_id': paragraph['document_id']\n",
    "        }\n",
    "        augmented_paragraphs.append(augmented_paragraph)\n",
    "\n",
    "    augmented_entry = {'paragraphs': augmented_paragraphs}\n",
    "    augmented_data.append(augmented_entry)\n",
    "\n",
    "# Save the augmented data to a JSON file\n",
    "with open('/Users/yangl/Downloads/augmented_data_3.json', 'w') as file:\n",
    "    json.dump(augmented_data, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random deletion \n",
    "\n",
    "import json\n",
    "import nltk\n",
    "import random\n",
    "\n",
    "# Read the input data from a JSON file\n",
    "with open('/Users/yangl/Downloads/answers.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Perform data augmentation for each entry\n",
    "augmented_data = []\n",
    "for entry in data['data']:\n",
    "    paragraphs = entry['paragraphs']\n",
    "    augmented_paragraphs = []\n",
    "    for paragraph in paragraphs:\n",
    "        qas = paragraph['qas']\n",
    "        augmented_qas = []\n",
    "        augmented_questions_set = set()  # To store unique augmented questions\n",
    "        for qa in qas:\n",
    "            original_question = qa['question']\n",
    "            answer = qa['answers'][0]['text']\n",
    "\n",
    "            # Tokenize the original question\n",
    "            tokens = nltk.word_tokenize(original_question)\n",
    "\n",
    "            # Augment the question by randomly deleting a word\n",
    "            augmented_tokens = tokens.copy()\n",
    "            if len(tokens) > 1:\n",
    "                # Randomly determine the number of words to delete (up to a maximum of half the words)\n",
    "                num_deletions = random.randint(1, min(len(tokens) // 2, 5))\n",
    "                for _ in range(num_deletions):\n",
    "                    if augmented_tokens:\n",
    "                        word_index = random.randint(0, len(augmented_tokens) - 1)\n",
    "                        augmented_tokens.pop(word_index)\n",
    "\n",
    "            # Reconstruct the augmented question\n",
    "            augmented_question = ' '.join(augmented_tokens)\n",
    "\n",
    "            if augmented_question and augmented_question not in augmented_questions_set:\n",
    "                augmented_questions_set.add(augmented_question)\n",
    "\n",
    "                augmented_qa = {\n",
    "                    'question': original_question,\n",
    "                    'augmented_questions': [augmented_question],\n",
    "                    'answers': [{'text': answer}]\n",
    "                }\n",
    "                augmented_qas.append(augmented_qa)\n",
    "\n",
    "        augmented_paragraph = {\n",
    "            'qas': augmented_qas,\n",
    "            'context': paragraph['context'],\n",
    "            'document_id': paragraph['document_id']\n",
    "        }\n",
    "        augmented_paragraphs.append(augmented_paragraph)\n",
    "\n",
    "    augmented_entry = {'paragraphs': augmented_paragraphs}\n",
    "    augmented_data.append(augmented_entry)\n",
    "\n",
    "# Save the augmented data to a JSON file\n",
    "with open('/Users/yangl/Downloads/augmented_data_5.json', 'w') as file:\n",
    "    json.dump(augmented_data, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random deletion with sentence shuffling\n",
    "import json\n",
    "import nltk\n",
    "import random\n",
    "\n",
    "# Read the input data from a JSON file\n",
    "with open('/Users/yangl/Downloads/answers.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Perform data augmentation for each entry\n",
    "augmented_data = []\n",
    "for entry in data['data']:\n",
    "    paragraphs = entry['paragraphs']\n",
    "    augmented_paragraphs = []\n",
    "    for paragraph in paragraphs:\n",
    "        qas = paragraph['qas']\n",
    "        augmented_qas = []\n",
    "        augmented_questions_set = set()  # To store unique augmented questions\n",
    "        for qa in qas:\n",
    "            original_question = qa['question']\n",
    "            answer = qa['answers'][0]['text']\n",
    "\n",
    "            # Tokenize the original question\n",
    "            tokens = nltk.word_tokenize(original_question)\n",
    "\n",
    "            # Augment the question by randomly deleting a word\n",
    "            augmented_tokens = tokens.copy()\n",
    "            if len(tokens) > 1:\n",
    "                # Randomly determine the number of words to delete (up to a maximum of half the words)\n",
    "                num_deletions = random.randint(1, min(len(tokens) // 2, 5))\n",
    "                for _ in range(num_deletions):\n",
    "                    if augmented_tokens:\n",
    "                        word_index = random.randint(0, len(augmented_tokens) - 1)\n",
    "                        augmented_tokens.pop(word_index)\n",
    "\n",
    "            # Reconstruct the augmented question\n",
    "            augmented_question = ' '.join(augmented_tokens)\n",
    "\n",
    "            if augmented_question and augmented_question not in augmented_questions_set:\n",
    "                augmented_questions_set.add(augmented_question)\n",
    "\n",
    "                # Augment the answer by shuffling the words\n",
    "                answer_tokens = nltk.word_tokenize(answer)\n",
    "                random.shuffle(answer_tokens)\n",
    "                augmented_answer = ' '.join(answer_tokens)\n",
    "\n",
    "                # Check if the augmented answer makes sense\n",
    "                if nltk.word_tokenize(augmented_answer) != answer_tokens:\n",
    "                    # Augmented answer does not make sense, skip this augmentation\n",
    "                    continue\n",
    "\n",
    "                augmented_qa = {\n",
    "                    'question': original_question,\n",
    "                    'augmented_questions': [augmented_question],\n",
    "                    'answers': [{'text': augmented_answer}]\n",
    "                }\n",
    "                augmented_qas.append(augmented_qa)\n",
    "\n",
    "        augmented_paragraph = {\n",
    "            'qas': augmented_qas,\n",
    "            'context': paragraph['context'],\n",
    "            'document_id': paragraph['document_id']\n",
    "        }\n",
    "        augmented_paragraphs.append(augmented_paragraph)\n",
    "\n",
    "    augmented_entry = {'paragraphs': augmented_paragraphs}\n",
    "    augmented_data.append(augmented_entry)\n",
    "\n",
    "# Save the augmented data to a JSON file\n",
    "with open('/Users/yangl/Downloads/augmented_data_5.json', 'w') as file:\n",
    "    json.dump(augmented_data, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random word insertion\n",
    "\n",
    "import json\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "import random\n",
    "\n",
    "# Read the input data from a JSON file\n",
    "with open('/Users/yangl/Downloads/answers.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Perform data augmentation for each entry\n",
    "augmented_data = []\n",
    "for entry in data['data']:\n",
    "    paragraphs = entry['paragraphs']\n",
    "    augmented_paragraphs = []\n",
    "    for paragraph in paragraphs:\n",
    "        qas = paragraph['qas']\n",
    "        augmented_qas = []\n",
    "        for qa in qas:\n",
    "            original_question = qa['question']\n",
    "            answer = qa['answers'][0]['text']\n",
    "\n",
    "            # Tokenize the original question\n",
    "            tokens = nltk.word_tokenize(original_question)\n",
    "\n",
    "            # Augment the question by replacing a random word with a synonym\n",
    "            augmented_tokens = tokens.copy()\n",
    "            replaced_words = set()  # Set to store replaced words\n",
    "            for i, word in enumerate(tokens):\n",
    "                synsets = wordnet.synsets(word)\n",
    "                if synsets:\n",
    "                    synonyms = []\n",
    "                    for synset in synsets:\n",
    "                        for lemma in synset.lemmas():\n",
    "                            synonyms.append(lemma.name())\n",
    "                    if synonyms:\n",
    "                        new_word = random.choice(synonyms)\n",
    "                        if new_word not in replaced_words:  # Avoid duplicates\n",
    "                            augmented_tokens[i] = new_word\n",
    "                            replaced_words.add(new_word)\n",
    "\n",
    "            # Reconstruct the augmented question\n",
    "            augmented_question = ' '.join(augmented_tokens)\n",
    "\n",
    "            augmented_qa = {\n",
    "                'question': original_question,\n",
    "                'augmented_questions': [augmented_question],\n",
    "                'answers': [{'text': answer}]\n",
    "            }\n",
    "            augmented_qas.append(augmented_qa)\n",
    "\n",
    "        augmented_paragraph = {\n",
    "            'qas': augmented_qas,\n",
    "            'context': paragraph['context'],\n",
    "            'document_id': paragraph['document_id']\n",
    "        }\n",
    "        augmented_paragraphs.append(augmented_paragraph)\n",
    "\n",
    "    augmented_entry = {'paragraphs': augmented_paragraphs}\n",
    "    augmented_data.append(augmented_entry)\n",
    "# Save the augmented data to a JSON file\n",
    "with open('/Users/yangl/Downloads/augmented_data_4.json', 'w') as file:\n",
    "    json.dump(augmented_data, file, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random word insertion and sentense shuffling \n",
    "import json\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "import random\n",
    "\n",
    "# Read the input data from a JSON file\n",
    "with open('/Users/yangl/Downloads/answers.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Perform data augmentation for each entry\n",
    "augmented_data = []\n",
    "for entry in data['data']:\n",
    "    paragraphs = entry['paragraphs']\n",
    "    augmented_paragraphs = []\n",
    "    for paragraph in paragraphs:\n",
    "        qas = paragraph['qas']\n",
    "        augmented_qas = []\n",
    "        for qa in qas:\n",
    "            original_question = qa['question']\n",
    "            answer = qa['answers'][0]['text']\n",
    "\n",
    "            # Tokenize the original question\n",
    "            question_tokens = nltk.word_tokenize(original_question)\n",
    "\n",
    "            # Augment the question by replacing a random word with a synonym\n",
    "            augmented_question_tokens = question_tokens.copy()\n",
    "            replaced_words = set()  # Set to store replaced words\n",
    "            for i, word in enumerate(question_tokens):\n",
    "                synsets = wordnet.synsets(word)\n",
    "                if synsets:\n",
    "                    synonyms = []\n",
    "                    for synset in synsets:\n",
    "                        for lemma in synset.lemmas():\n",
    "                            synonyms.append(lemma.name())\n",
    "                    if synonyms:\n",
    "                        new_word = random.choice(synonyms)\n",
    "                        if new_word not in replaced_words:  # Avoid duplicates\n",
    "                            augmented_question_tokens.insert(i, new_word)\n",
    "                            replaced_words.add(new_word)\n",
    "\n",
    "            # Reconstruct the augmented question\n",
    "            augmented_question = ' '.join(augmented_question_tokens)\n",
    "\n",
    "            # Shuffle the words in the answer sentence\n",
    "            answer_tokens = nltk.word_tokenize(answer)\n",
    "            random.shuffle(answer_tokens)\n",
    "            augmented_answer = ' '.join(answer_tokens)\n",
    "\n",
    "            # Check if the augmented question and answer make sense\n",
    "            if nltk.word_tokenize(augmented_question) != augmented_question_tokens or \\\n",
    "                    nltk.word_tokenize(augmented_answer) != answer_tokens:\n",
    "                # Augmented question or answer does not make sense, skip this augmentation\n",
    "                continue\n",
    "\n",
    "            augmented_qa = {\n",
    "                'question': original_question,\n",
    "                'augmented_questions': [augmented_question],\n",
    "                'answers': [{'text': augmented_answer}]\n",
    "            }\n",
    "            augmented_qas.append(augmented_qa)\n",
    "\n",
    "        augmented_paragraph = {\n",
    "            'qas': augmented_qas,\n",
    "            'context': paragraph['context'],\n",
    "            'document_id': paragraph['document_id']\n",
    "        }\n",
    "        augmented_paragraphs.append(augmented_paragraph)\n",
    "\n",
    "    augmented_entry = {'paragraphs': augmented_paragraphs}\n",
    "    augmented_data.append(augmented_entry)\n",
    "\n",
    "# Save the augmented data to a JSON file\n",
    "with open('/Users/yangl/Downloads/augmented_data_4.json', 'w') as file:\n",
    "    json.dump(augmented_data, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#randon swap \n",
    "import json\n",
    "import random\n",
    "\n",
    "# Read the input data from a JSON file\n",
    "with open('/Users/yangl/Downloads/answers.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Perform data augmentation for each entry\n",
    "augmented_data = []\n",
    "for entry in data['data']:\n",
    "    paragraphs = entry['paragraphs']\n",
    "    augmented_paragraphs = []\n",
    "    for paragraph in paragraphs:\n",
    "        qas = paragraph['qas']\n",
    "        augmented_qas = []\n",
    "        augmented_questions_set = set()  # To store unique augmented questions\n",
    "        for qa in qas:\n",
    "            original_question = qa['question']\n",
    "            answer = qa['answers'][0]['text']\n",
    "\n",
    "            # Tokenize the original question\n",
    "            tokens = original_question.split()\n",
    "\n",
    "            # Augment the question by randomly swapping adjacent words\n",
    "            augmented_questions = set()\n",
    "            while len(augmented_questions) < 1:  # Generate at most 1 unique augmented question\n",
    "                augmented_tokens = tokens.copy()\n",
    "                if len(tokens) > 1:\n",
    "                    for i in range(len(tokens) - 1):\n",
    "                        if random.random() < 0.2:  # 20% chance of swapping adjacent words\n",
    "                            augmented_tokens[i], augmented_tokens[i+1] = augmented_tokens[i+1], augmented_tokens[i]\n",
    "\n",
    "                # Reconstruct the augmented question\n",
    "                augmented_question = ' '.join(augmented_tokens)\n",
    "\n",
    "                if augmented_question != original_question and augmented_question not in augmented_questions_set:\n",
    "                    augmented_questions.add(augmented_question)\n",
    "                    augmented_questions_set.add(augmented_question)\n",
    "\n",
    "            if augmented_questions:\n",
    "                augmented_question = augmented_questions.pop()\n",
    "\n",
    "                augmented_qa = {\n",
    "                    'question': original_question,\n",
    "                    'augmented_questions': [augmented_question],\n",
    "                    'answers': [{'text': answer}]\n",
    "                }\n",
    "                augmented_qas.append(augmented_qa)\n",
    "\n",
    "        augmented_paragraph = {\n",
    "            'qas': augmented_qas,\n",
    "            'context': paragraph['context'],\n",
    "            'document_id': paragraph['document_id']\n",
    "        }\n",
    "        augmented_paragraphs.append(augmented_paragraph)\n",
    "\n",
    "    augmented_entry = {'paragraphs': augmented_paragraphs}\n",
    "    augmented_data.append(augmented_entry)\n",
    "    \n",
    "# Save the augmented data to a JSON file\n",
    "with open('/Users/yangl/Downloads/augmented_data_8.json', 'w') as file:\n",
    "    json.dump(augmented_data, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random swap and sentence shuffling for answer\n",
    "import json\n",
    "import random\n",
    "\n",
    "# Read the input data from a JSON file\n",
    "with open('/Users/yangl/Downloads/answers.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Perform data augmentation for each entry\n",
    "augmented_data = []\n",
    "for entry in data['data']:\n",
    "    paragraphs = entry['paragraphs']\n",
    "    augmented_paragraphs = []\n",
    "    for paragraph in paragraphs:\n",
    "        qas = paragraph['qas']\n",
    "        augmented_qas = []\n",
    "        augmented_questions_set = set()  # To store unique augmented questions\n",
    "        for qa in qas:\n",
    "            original_question = qa['question']\n",
    "            answer = qa['answers'][0]['text']\n",
    "\n",
    "            # Tokenize the original question\n",
    "            tokens = original_question.split()\n",
    "\n",
    "            # Augment the question by randomly swapping adjacent words\n",
    "            augmented_questions = set()\n",
    "            while len(augmented_questions) < 1:  # Generate at most 1 unique augmented question\n",
    "                augmented_tokens = tokens.copy()\n",
    "                if len(tokens) > 1:\n",
    "                    for i in range(len(tokens) - 1):\n",
    "                        if random.random() < 0.2:  # 20% chance of swapping adjacent words\n",
    "                            augmented_tokens[i], augmented_tokens[i+1] = augmented_tokens[i+1], augmented_tokens[i]\n",
    "\n",
    "                # Reconstruct the augmented question\n",
    "                augmented_question = ' '.join(augmented_tokens)\n",
    "\n",
    "                if augmented_question != original_question and augmented_question not in augmented_questions_set:\n",
    "                    augmented_questions.add(augmented_question)\n",
    "                    augmented_questions_set.add(augmented_question)\n",
    "\n",
    "            if augmented_questions:\n",
    "                augmented_question = augmented_questions.pop()\n",
    "\n",
    "                # Shuffle the words in the answer sentence\n",
    "                answer_tokens = answer.split()\n",
    "                random.shuffle(answer_tokens)\n",
    "                augmented_answer = ' '.join(answer_tokens)\n",
    "\n",
    "                # Check if the augmented question and answer make sense\n",
    "                if augmented_question != original_question and augmented_answer != answer:\n",
    "                    augmented_qa = {\n",
    "                        'question': original_question,\n",
    "                        'augmented_questions': [augmented_question],\n",
    "                        'answers': [{'text': augmented_answer}]\n",
    "                    }\n",
    "                    augmented_qas.append(augmented_qa)\n",
    "\n",
    "        augmented_paragraph = {\n",
    "            'qas': augmented_qas,\n",
    "            'context': paragraph['context'],\n",
    "            'document_id': paragraph['document_id']\n",
    "        }\n",
    "        augmented_paragraphs.append(augmented_paragraph)\n",
    "\n",
    "    augmented_entry = {'paragraphs': augmented_paragraphs}\n",
    "    augmented_data.append(augmented_entry)\n",
    "    \n",
    "# Save the augmented data to a JSON file\n",
    "with open('/Users/yangl/Downloads/augmented_data_8.json', 'w') as file:\n",
    "    json.dump(augmented_data, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Back translation draft\n",
    "\n",
    "import json\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "# Read the input data from a JSON file\n",
    "with open('/Users/yangl/Downloads/answers.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Initialize the back translation models and tokenizers\n",
    "src_lang = 'en'\n",
    "target_lang = 'fr'  # Specify the target language for back translation\n",
    "model_name = f'Helsinki-NLP/opus-mt-{src_lang}-{target_lang}'\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Perform data augmentation for each entry\n",
    "augmented_data = []\n",
    "for entry in data['data']:\n",
    "    paragraphs = entry['paragraphs']\n",
    "    augmented_paragraphs = []\n",
    "    for paragraph in paragraphs:\n",
    "        qas = paragraph['qas']\n",
    "        augmented_qas = []\n",
    "        for qa in qas:\n",
    "            original_question = qa['question']\n",
    "            answer = qa['answers'][0]['text']\n",
    "\n",
    "            # Forward translation: English to Target Language\n",
    "            encoded_input = tokenizer.encode(original_question, return_tensors='pt')\n",
    "            inputs = {\n",
    "                'input_ids': encoded_input.to(model.device),\n",
    "                'attention_mask': encoded_input.to(model.device),\n",
    "            }\n",
    "            translated = model.generate(**inputs, max_length=128, num_beams=5, num_return_sequences=1)\n",
    "            translated_question = tokenizer.decode(translated[0], skip_special_tokens=True)\n",
    "\n",
    "            # Back translation: Target Language to English\n",
    "            back_model_name = f'Helsinki-NLP/opus-mt-{target_lang}-{src_lang}'\n",
    "            back_model = MarianMTModel.from_pretrained(back_model_name)\n",
    "            back_tokenizer = MarianTokenizer.from_pretrained(back_model_name)\n",
    "\n",
    "            encoded_input = back_tokenizer.encode(translated_question, return_tensors='pt')\n",
    "            inputs = {\n",
    "                'input_ids': encoded_input.to(back_model.device),\n",
    "                'attention_mask': encoded_input.to(back_model.device),\n",
    "            }\n",
    "            back_translated = back_model.generate(**inputs, max_length=128, num_beams=5, num_return_sequences=1)\n",
    "            back_translated_question = back_tokenizer.decode(back_translated[0], skip_special_tokens=True)\n",
    "\n",
    "            augmented_questions = [original_question, back_translated_question]\n",
    "\n",
    "            augmented_qa = {\n",
    "                'question': original_question,\n",
    "                'augmented_questions': augmented_questions,\n",
    "                'answers': [{'text': answer}]\n",
    "            }\n",
    "            augmented_qas.append(augmented_qa)\n",
    "\n",
    "        augmented_paragraph = {\n",
    "            'qas': augmented_qas,\n",
    "            'context': paragraph['context'],\n",
    "            'document_id': paragraph['document_id']\n",
    "        }\n",
    "        augmented_paragraphs.append(augmented_paragraph)\n",
    "\n",
    "    augmented_entry = {'paragraphs': augmented_paragraphs}\n",
    "    augmented_data.append(augmented_entry)\n",
    "\n",
    "# Save the augmented data to a JSON file\n",
    "with open('/Users/yangl/Downloads/augmented_data_6.json', 'w', encoding='utf-8') as file:\n",
    "    json.dump(augmented_data, file, indent=4, ensure_ascii=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
