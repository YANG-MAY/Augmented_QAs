{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Wha is asthma?',\n",
       " 'What i asthma?',\n",
       " 'What is astha?',\n",
       " 'What s asthma?',\n",
       " 'Wht is asthma?']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textattack.transformations import WordSwapRandomCharacterDeletion\n",
    "from textattack.transformations import CompositeTransformation\n",
    "from textattack.augmentation import Augmenter\n",
    "transformation = CompositeTransformation([WordSwapRandomCharacterDeletion()])\n",
    "augmenter = Augmenter(transformation=transformation, transformations_per_example=5)\n",
    "s = 'What is asthma?'\n",
    "augmenter.augment(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to /Users/yangl/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['What is comprise asthma.', 'asthma is What.', 'is asthma.', 'What is asthma.']\n",
      "['follow What is asthma.', 'What is asthma.', 'asthma is What.', 'What asthma.']\n",
      "['is What asthma.', 'What is exist asthma.', 'What is asthma.', 'What is.']\n",
      "['is asthma.', 'What asthma is.', 'exist What is asthma.', 'What is asthma.']\n",
      "['comprise What is asthma.', 'What is asthma.', 'What is.', 'is What asthma.']\n",
      "['What is be asthma.', 'asthma is What.', 'What is.', 'What is asthma.']\n",
      "['What is live asthma.', 'asthma is What.', 'What is asthma.', 'is asthma.']\n",
      "['embody What is asthma.', 'What is asthma.', 'is What asthma.', 'What asthma.']\n",
      "['What is asthma.', 'What asthma is.', 'What is cost asthma.', 'is asthma.']\n",
      "['is What asthma.', 'What is.', 'What is asthma.', 'What be is asthma.']\n"
     ]
    }
   ],
   "source": [
    "from textattack.augmentation import EasyDataAugmenter\n",
    "augmenter = EasyDataAugmenter()\n",
    "q = 'What is asthma.'\n",
    "iteration = [1,2,3,4,5,6,7,8,9,10]\n",
    "for x in iteration:\n",
    "    print(augmenter.augment(q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'CheckListAugmenter' object has no attribute 'add_transformation'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[104], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39m# Initialize the ChecklistAugmenter\u001b[39;00m\n\u001b[1;32m     10\u001b[0m augmenter \u001b[39m=\u001b[39m CheckListAugmenter()\n\u001b[0;32m---> 12\u001b[0m augmenter\u001b[39m.\u001b[39;49madd_transformation(WordNetAugmenter())\n\u001b[1;32m     15\u001b[0m \u001b[39m# Perform data augmentation for each entry\u001b[39;00m\n\u001b[1;32m     16\u001b[0m augmented_data \u001b[39m=\u001b[39m []\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'CheckListAugmenter' object has no attribute 'add_transformation'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from textattack.augmentation import CheckListAugmenter\n",
    "from textattack.augmentation import WordNetAugmenter\n",
    "\n",
    "# Read the input data from a JSON file\n",
    "with open('/Users/yangl/Downloads/answers.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Initialize the ChecklistAugmenter\n",
    "augmenter = CheckListAugmenter()\n",
    "\n",
    "augmenter.add_transformation(WordNetAugmenter())\n",
    "\n",
    "\n",
    "# Perform data augmentation for each entry\n",
    "augmented_data = []\n",
    "for entry in data['data']:\n",
    "    paragraphs = entry['paragraphs']\n",
    "    augmented_paragraphs = []\n",
    "    for paragraph in paragraphs:\n",
    "        qas = paragraph['qas']\n",
    "        augmented_qas = []\n",
    "        for qa in qas:\n",
    "            original_question = qa['question']\n",
    "            answer = qa['answers'][0]['text']\n",
    "\n",
    "            augmented_questions = augmenter.augment(original_question)\n",
    "\n",
    "            augmented_qa = {\n",
    "                'question': original_question,\n",
    "                'augmented_questions': augmented_questions,\n",
    "                'answers': [{'text': answer}]\n",
    "            }\n",
    "            augmented_qas.append(augmented_qa)\n",
    "\n",
    "        augmented_paragraph = {\n",
    "            'qas': augmented_qas,\n",
    "            'context': paragraph['context'],\n",
    "            'document_id': paragraph['document_id']\n",
    "        }\n",
    "        augmented_paragraphs.append(augmented_paragraph)\n",
    "\n",
    "    augmented_entry = {'paragraphs': augmented_paragraphs}\n",
    "    augmented_data.append(augmented_entry)\n",
    "\n",
    "# Save the augmented data to a new JSON file\n",
    "with open('/Users/yangl/Downloads/augmented_data_3.json', 'w') as file:\n",
    "    json.dump(augmented_data, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Synonym Replacement\n",
    "\n",
    "\n",
    "import json\n",
    "import nlpaug.augmenter.word as naw\n",
    "\n",
    "# Read the input data from a JSON file\n",
    "with open('/Users/yangl/Downloads/answers.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Perform data augmentation for each entry\n",
    "augmented_data = []\n",
    "for entry in data['data']:\n",
    "    paragraphs = entry['paragraphs']\n",
    "    augmented_paragraphs = []\n",
    "    for paragraph in paragraphs:\n",
    "        qas = paragraph['qas']\n",
    "        augmented_qas = []\n",
    "        for qa in qas:\n",
    "            original_question = qa['question']\n",
    "            answer = qa['answers'][0]['text']\n",
    "\n",
    "            # Apply Synonym Replacement using nlpaug\n",
    "            augmenter = naw.SynonymAug(aug_src='wordnet')\n",
    "            augmented_questions = augmenter.augment(original_question, n=5)\n",
    "\n",
    "            augmented_qa = {\n",
    "                'question': original_question,\n",
    "                'augmented_questions': augmented_questions,\n",
    "                'answers': [{'text': answer}]\n",
    "            }\n",
    "            augmented_qas.append(augmented_qa)\n",
    "\n",
    "        augmented_paragraph = {\n",
    "            'qas': augmented_qas,\n",
    "            'context': paragraph['context'],\n",
    "            'document_id': paragraph['document_id']\n",
    "        }\n",
    "        augmented_paragraphs.append(augmented_paragraph)\n",
    "\n",
    "    augmented_entry = {'paragraphs': augmented_paragraphs}\n",
    "    augmented_data.append(augmented_entry)\n",
    "    \n",
    "    with open('/Users/yangl/Downloads/augmented_data_3.json', 'w') as file:\n",
    "        json.dump(augmented_data, file, indent=4)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'.'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[125], line 35\u001b[0m\n\u001b[1;32m     32\u001b[0m pos \u001b[39m=\u001b[39m nltk\u001b[39m.\u001b[39mpos_tag(tokens)[word_index][\u001b[39m1\u001b[39m]\n\u001b[1;32m     34\u001b[0m \u001b[39m# Get synsets (related words) for the selected word\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m synsets \u001b[39m=\u001b[39m wordnet\u001b[39m.\u001b[39;49msynsets(original_word, pos\u001b[39m=\u001b[39;49mpos)\n\u001b[1;32m     37\u001b[0m \u001b[39mif\u001b[39;00m synsets:\n\u001b[1;32m     38\u001b[0m     \u001b[39m# Get the first synset (related word)\u001b[39;00m\n\u001b[1;32m     39\u001b[0m     related_word \u001b[39m=\u001b[39m synsets[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mlemmas()[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mname()\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/nltk/corpus/reader/wordnet.py:1757\u001b[0m, in \u001b[0;36mWordNetCorpusReader.synsets\u001b[0;34m(self, lemma, pos, lang, check_exceptions)\u001b[0m\n\u001b[1;32m   1755\u001b[0m     \u001b[39mif\u001b[39;00m pos \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1756\u001b[0m         pos \u001b[39m=\u001b[39m POS_LIST\n\u001b[0;32m-> 1757\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[1;32m   1758\u001b[0m         get_synset(p, offset)\n\u001b[1;32m   1759\u001b[0m         \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m pos\n\u001b[1;32m   1760\u001b[0m         \u001b[39mfor\u001b[39;00m form \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_morphy(lemma, p, check_exceptions)\n\u001b[1;32m   1761\u001b[0m         \u001b[39mfor\u001b[39;00m offset \u001b[39min\u001b[39;00m index[form]\u001b[39m.\u001b[39mget(p, [])\n\u001b[1;32m   1762\u001b[0m     ]\n\u001b[1;32m   1764\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1765\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_load_lang_data(lang)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/nltk/corpus/reader/wordnet.py:1760\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1755\u001b[0m     \u001b[39mif\u001b[39;00m pos \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1756\u001b[0m         pos \u001b[39m=\u001b[39m POS_LIST\n\u001b[1;32m   1757\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[1;32m   1758\u001b[0m         get_synset(p, offset)\n\u001b[1;32m   1759\u001b[0m         \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m pos\n\u001b[0;32m-> 1760\u001b[0m         \u001b[39mfor\u001b[39;00m form \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_morphy(lemma, p, check_exceptions)\n\u001b[1;32m   1761\u001b[0m         \u001b[39mfor\u001b[39;00m offset \u001b[39min\u001b[39;00m index[form]\u001b[39m.\u001b[39mget(p, [])\n\u001b[1;32m   1762\u001b[0m     ]\n\u001b[1;32m   1764\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1765\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_load_lang_data(lang)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.10/site-packages/nltk/corpus/reader/wordnet.py:2072\u001b[0m, in \u001b[0;36mWordNetCorpusReader._morphy\u001b[0;34m(self, form, pos, check_exceptions)\u001b[0m\n\u001b[1;32m   2064\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_morphy\u001b[39m(\u001b[39mself\u001b[39m, form, pos, check_exceptions\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m   2065\u001b[0m     \u001b[39m# from jordanbg:\u001b[39;00m\n\u001b[1;32m   2066\u001b[0m     \u001b[39m# Given an original string x\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2069\u001b[0m     \u001b[39m# 3. If there are no matches, keep applying rules until you either\u001b[39;00m\n\u001b[1;32m   2070\u001b[0m     \u001b[39m#    find a match or you can't go any further\u001b[39;00m\n\u001b[0;32m-> 2072\u001b[0m     exceptions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_exception_map[pos]\n\u001b[1;32m   2073\u001b[0m     substitutions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mMORPHOLOGICAL_SUBSTITUTIONS[pos]\n\u001b[1;32m   2075\u001b[0m     \u001b[39mdef\u001b[39;00m \u001b[39mapply_rules\u001b[39m(forms):\n",
      "\u001b[0;31mKeyError\u001b[0m: '.'"
     ]
    }
   ],
   "source": [
    "# random deletion \n",
    "\n",
    "import json\n",
    "import nltk\n",
    "import random\n",
    "\n",
    "# Read the input data from a JSON file\n",
    "with open('/Users/yangl/Downloads/answers.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Perform data augmentation for each entry\n",
    "augmented_data = []\n",
    "for entry in data['data']:\n",
    "    paragraphs = entry['paragraphs']\n",
    "    augmented_paragraphs = []\n",
    "    for paragraph in paragraphs:\n",
    "        qas = paragraph['qas']\n",
    "        augmented_qas = []\n",
    "        augmented_questions_set = set()  # To store unique augmented questions\n",
    "        for qa in qas:\n",
    "            original_question = qa['question']\n",
    "            answer = qa['answers'][0]['text']\n",
    "\n",
    "            # Tokenize the original question\n",
    "            tokens = nltk.word_tokenize(original_question)\n",
    "\n",
    "            # Augment the question by randomly deleting a word\n",
    "            augmented_tokens = tokens.copy()\n",
    "            if len(tokens) > 1:\n",
    "                word_index = random.randint(0, len(tokens) - 1)\n",
    "                original_word = augmented_tokens[word_index]\n",
    "\n",
    "                # Get the part of speech of the word\n",
    "                pos = nltk.pos_tag(tokens)[word_index][1]\n",
    "\n",
    "                # Get synsets (related words) for the selected word\n",
    "                synsets = wordnet.synsets(original_word, pos=pos)\n",
    "\n",
    "                if synsets:\n",
    "                    # Get the first synset (related word)\n",
    "                    related_word = synsets[0].lemmas()[0].name()\n",
    "\n",
    "                    # Replace the selected word with the related word\n",
    "                    augmented_tokens[word_index] = related_word\n",
    "\n",
    "            # Reconstruct the augmented question\n",
    "            augmented_question = ' '.join(augmented_tokens)\n",
    "\n",
    "            if augmented_question and augmented_question not in augmented_questions_set:\n",
    "                augmented_questions_set.add(augmented_question)\n",
    "\n",
    "                augmented_qa = {\n",
    "                    'question': original_question,\n",
    "                    'augmented_questions': [augmented_question],\n",
    "                    'answers': [{'text': answer}]\n",
    "                }\n",
    "                augmented_qas.append(augmented_qa)\n",
    "\n",
    "        augmented_paragraph = {\n",
    "            'qas': augmented_qas,\n",
    "            'context': paragraph['context'],\n",
    "            'document_id': paragraph['document_id']\n",
    "        }\n",
    "        augmented_paragraphs.append(augmented_paragraph)\n",
    "\n",
    "    augmented_entry = {'paragraphs': augmented_paragraphs}\n",
    "    augmented_data.append(augmented_entry)\n",
    "\n",
    "# Save the augmented data to a JSON file\n",
    "with open('/Users/yangl/Downloads/augmented_data_5.json', 'w') as file:\n",
    "    json.dump(augmented_data, file, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final random word insertion\n",
    "\n",
    "import json\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "import random\n",
    "\n",
    "# Read the input data from a JSON file\n",
    "with open('/Users/yangl/Downloads/answers.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Perform data augmentation for each entry\n",
    "augmented_data = []\n",
    "for entry in data['data']:\n",
    "    paragraphs = entry['paragraphs']\n",
    "    augmented_paragraphs = []\n",
    "    for paragraph in paragraphs:\n",
    "        qas = paragraph['qas']\n",
    "        augmented_qas = []\n",
    "        for qa in qas:\n",
    "            original_question = qa['question']\n",
    "            answer = qa['answers'][0]['text']\n",
    "\n",
    "            # Tokenize the original question\n",
    "            tokens = nltk.word_tokenize(original_question)\n",
    "\n",
    "            # Augment the question by replacing a random word with a synonym\n",
    "            augmented_tokens = tokens.copy()\n",
    "            replaced_words = set()  # Set to store replaced words\n",
    "            for i, word in enumerate(tokens):\n",
    "                synsets = wordnet.synsets(word)\n",
    "                if synsets:\n",
    "                    synonyms = []\n",
    "                    for synset in synsets:\n",
    "                        for lemma in synset.lemmas():\n",
    "                            synonyms.append(lemma.name())\n",
    "                    if synonyms:\n",
    "                        new_word = random.choice(synonyms)\n",
    "                        if new_word not in replaced_words:  # Avoid duplicates\n",
    "                            augmented_tokens[i] = new_word\n",
    "                            replaced_words.add(new_word)\n",
    "\n",
    "            # Reconstruct the augmented question\n",
    "            augmented_question = ' '.join(augmented_tokens)\n",
    "\n",
    "            augmented_qa = {\n",
    "                'question': original_question,\n",
    "                'augmented_questions': [augmented_question],\n",
    "                'answers': [{'text': answer}]\n",
    "            }\n",
    "            augmented_qas.append(augmented_qa)\n",
    "\n",
    "        augmented_paragraph = {\n",
    "            'qas': augmented_qas,\n",
    "            'context': paragraph['context'],\n",
    "            'document_id': paragraph['document_id']\n",
    "        }\n",
    "        augmented_paragraphs.append(augmented_paragraph)\n",
    "\n",
    "    augmented_entry = {'paragraphs': augmented_paragraphs}\n",
    "    augmented_data.append(augmented_entry)\n",
    "# Save the augmented data to a JSON file\n",
    "with open('/Users/yangl/Downloads/augmented_data_4.json', 'w') as file:\n",
    "    json.dump(augmented_data, file, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import nltk\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "\n",
    "\n",
    "# Read the input data from a JSON file\n",
    "with open('/Users/yangl/Downloads/answers.json', 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "\n",
    "# Initialize the back translation models and tokenizers\n",
    "src_lang = 'en'\n",
    "target_lang = 'fr'  # Specify the target language for back translation\n",
    "model_name = f'Helsinki-NLP/opus-mt-{src_lang}-{target_lang}'\n",
    "model = MarianMTModel.from_pretrained(model_name)\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Perform data augmentation for each entry\n",
    "augmented_data = []\n",
    "for entry in data['data']:\n",
    "    paragraphs = entry['paragraphs']\n",
    "    augmented_paragraphs = []\n",
    "    for paragraph in paragraphs:\n",
    "        qas = paragraph['qas']\n",
    "        augmented_qas = []\n",
    "        for qa in qas:\n",
    "            original_question = qa['question']\n",
    "            answer = qa['answers'][0]['text']\n",
    "\n",
    "            # Perform back translation\n",
    "            inputs = tokenizer(original_question, truncation=True, padding='longest', return_tensors='pt')\n",
    "            translated = model.generate(**inputs, max_length=128, num_beams=5, num_return_sequences=1)\n",
    "            back_translated = [tokenizer.decode(t, skip_special_tokens=True) for t in translated]\n",
    "            augmented_questions = [original_question] + back_translated\n",
    "\n",
    "            augmented_qa = {\n",
    "                'question': original_question,\n",
    "                'augmented_questions': augmented_questions,\n",
    "                'answers': [{'text': answer}]\n",
    "            }\n",
    "            augmented_qas.append(augmented_qa)\n",
    "\n",
    "        augmented_paragraph = {\n",
    "            'qas': augmented_qas,\n",
    "            'context': paragraph['context'],\n",
    "            'document_id': paragraph['document_id']\n",
    "        }\n",
    "        augmented_paragraphs.append(augmented_paragraph)\n",
    "\n",
    "    augmented_entry = {'paragraphs': augmented_paragraphs}\n",
    "    augmented_data.append(augmented_entry)\n",
    "\n",
    "\n",
    "# Save the augmented data to a JSON file\n",
    "with open('/Users/yangl/Downloads/augmented_data_6.json', 'w') as file:\n",
    "    json.dump(augmented_data, file, indent=4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
